{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# using padding \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the tokenizer\n",
    "# reference: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "tokenizer = Tokenizer(num_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(tokenizer, sentences,on_text=True):\n",
    "    # encode the words using the tokenizer\n",
    "    if on_text:\n",
    "        tokenizer.fit_on_texts(sentences)\n",
    "    else:\n",
    "        return tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index(tokenizer):\n",
    "    # get the indices of the words \n",
    "    return tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "fit_tokenizer(tokenizer, sentences)\n",
    "print(get_word_index(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.append('You love my dog!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "fit_tokenizer(tokenizer, sentences)\n",
    "print(get_word_index(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.append('Do you think my dog is amazing?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = fit_tokenizer(tokenizer, sentences, on_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "print(get_word_index(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 2, 4], [3, 1, 2, 5], [6, 1, 2, 4], [6, 2, 4]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrate example where words are not indexed \n",
    "def demonstrate_missing_words(tokenizer):\n",
    "    test_data = [\"i really love my dog\",  # really is not in the word index\n",
    "                 \"my dog loves my manatee\" # loves and manatee are not in either :( \n",
    "                ] \n",
    "    return fit_tokenizer(tokenizer, test_data, on_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_wrong = demonstrate_missing_words(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n",
      "[[3, 1, 2, 4], [2, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(get_word_index(tokenizer))\n",
    "print(sequences_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try an example where we don't ignore missing words \n",
    "# using oov token\n",
    "'''\n",
    "oov_token, if given, it will be added to word_index and used to\n",
    "    replace out-of-vocabulary words during text_to_sequence calls\n",
    "'''\n",
    "tokenizer_oov = Tokenizer(num_words=100, oov_token=\"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "# encode the words\n",
    "fit_tokenizer(tokenizer_oov, sentences)\n",
    "print(get_word_index(tokenizer_oov))\n",
    "# generate the sequences \n",
    "sequences = fit_tokenizer(tokenizer_oov, sentences, on_text=False)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = demonstrate_missing_words(tokenizer_oov)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love my dog',\n",
       " 'I, love my cat',\n",
       " 'You love my dog!',\n",
       " 'Do you think my dog is amazing?']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use padding :) \n",
    "# We will do the same as before but adding padding \n",
    "\n",
    "# encode the words\n",
    "fit_tokenizer(tokenizer, sentences)\n",
    "\n",
    "# generate the sequences\n",
    "seq_no_padding = fit_tokenizer(tokenizer, sentences, on_text=False)\n",
    "\n",
    "# adding the padding \n",
    "padded_seq = pad_sequences(seq_no_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "[[3, 2, 1, 4], [3, 2, 1, 5], [6, 2, 1, 4], [7, 6, 8, 1, 4, 9, 10]]\n",
      "[[ 0  0  0  3  2  1  4]\n",
      " [ 0  0  0  3  2  1  5]\n",
      " [ 0  0  0  6  2  1  4]\n",
      " [ 7  6  8  1  4  9 10]]\n"
     ]
    }
   ],
   "source": [
    "print(get_word_index(tokenizer))\n",
    "print(seq_no_padding)\n",
    "print(padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Word index: {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "\n",
      " Sequences [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      " Padded Sequences\n",
      "[[ 0  3  2  1  4]\n",
      " [ 0  3  2  1  5]\n",
      " [ 0  6  2  1  4]\n",
      " [ 8  1  4  9 10]]\n"
     ]
    }
   ],
   "source": [
    "# Now let's try to use it with different parameters \n",
    "tokenizer_oov = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "\n",
    "# encode the words\n",
    "fit_tokenizer(tokenizer_oov, sentences)\n",
    "print(\"\\n Word index: {}\".format(get_word_index(tokenizer_oov)))\n",
    "\n",
    "# generate the sequences \n",
    "sequences = fit_tokenizer(tokenizer_oov, sentences, on_text=False)\n",
    "print(\"\\n Sequences {}\".format(sequences))\n",
    "\n",
    "# adding the padding \n",
    "padded_seq = pad_sequences(seq_no_padding, maxlen=5)\n",
    "print(\"\\n Padded Sequences\\n{}\".format(padded_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Sequences\n",
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
      "[[0 0 0 0 0 5 1 3 2 4]\n",
      " [0 0 0 0 0 2 4 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# let's test it \n",
    "test_seq = demonstrate_missing_words(tokenizer_oov)\n",
    "print(\"\\n Test Sequences\\n{}\".format(test_seq))\n",
    "\n",
    "# adding the padding \n",
    "padded_seq = pad_sequences(test_seq, maxlen=10)\n",
    "print(padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4]\n",
      " [2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Only with 2\n",
    "# adding the padding \n",
    "padded_seq = pad_sequences(test_seq, maxlen=2)\n",
    "print(padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
