{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shakespeare_using_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1bjG30KFZ6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import numpy as np \n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxiEHSX-FtFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBstnT8hH5b-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rr5p-eRHRQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try: \n",
        "  from google.colab import files\n",
        "  BASE_PATH = \"/content\"\n",
        "except: \n",
        "  BASE_PATH = \"./datasets\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoBmT4bqGiaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "DATASET_NAME = \"shakespeare.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-fkhYUPGwTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_handler = tf.keras.utils.get_file(DATASET_NAME, DATASET_URL, cache_dir=BASE_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa2emEBIIBqS",
        "colab_type": "code",
        "outputId": "be0cf3d9-78dc-4053-c33a-e13d4a577938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Read the file and decode it \n",
        "text = open(file_handler, 'rb').read().decode(encoding='utf=8')\n",
        "\n",
        "# print number of characters on it\n",
        "print(\"Length of text: {} characters\".format(len(text)))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BoQRnXvIkyw",
        "colab_type": "code",
        "outputId": "afa5e17a-abd2-48c9-91de-aa0e85e0fbe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "# check first 250 chars\n",
        "print(text[:250])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdRy275cIqjx",
        "colab_type": "code",
        "outputId": "8974ad01-176e-4012-ff6a-b6b9a1dfc97f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Get the unique characters in the text \n",
        "vocab = sorted(set(text))\n",
        "print(\"{} unique characters\".format(len(vocab)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNZLTM-DJGG1",
        "colab_type": "text"
      },
      "source": [
        "# Process the text \n",
        "\n",
        "Let's vectorize the text by mapping to numerical representation. \n",
        "\n",
        "For doing so, we will create 2 maps:\n",
        "\n",
        "\n",
        "*   chars to nums\n",
        "*   nums to chars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcV6KQp2JdKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars_to_nums = { char:idx for idx, char in enumerate(vocab)}\n",
        "nums_to_char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([chars_to_nums[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdkCmVpbJ-oX",
        "colab_type": "code",
        "outputId": "f774d67b-6764-4479-f7cc-c46587304337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "# Print the representation \n",
        "print(\"{\")\n",
        "for char, _ in zip(chars_to_nums, range(20)):\n",
        "  print(\" {:4s}: {:3d},\".format(repr(char), chars_to_nums[char]))\n",
        "print(\" ...\\n}\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            " '\\n':   0,\n",
            " ' ' :   1,\n",
            " '!' :   2,\n",
            " '$' :   3,\n",
            " '&' :   4,\n",
            " \"'\" :   5,\n",
            " ',' :   6,\n",
            " '-' :   7,\n",
            " '.' :   8,\n",
            " '3' :   9,\n",
            " ':' :  10,\n",
            " ';' :  11,\n",
            " '?' :  12,\n",
            " 'A' :  13,\n",
            " 'B' :  14,\n",
            " 'C' :  15,\n",
            " 'D' :  16,\n",
            " 'E' :  17,\n",
            " 'F' :  18,\n",
            " 'G' :  19,\n",
            " ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRFCVCHYKr_w",
        "colab_type": "code",
        "outputId": "28ff7871-29ff-4773-98ed-938deb902e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Show how the first 13 chars from text are mapped to integers\n",
        "print(\"{} ----- characters mapped to int ----> {}\".format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ----- characters mapped to int ----> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyXzq_HSLMyM",
        "colab_type": "text"
      },
      "source": [
        "# Prediction Time \n",
        "\n",
        "Given an character or sequence of characters, what is the most probable next character ? \n",
        "\n",
        "We will use an RNN for this, since it mantains an internal state of what it has seen previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75xJYNG_Lkyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Max length of the sentence we want for a single input in characters\n",
        "seq_length = 100 \n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rZ9C6o2L822",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training examples \n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7g9wcYGMg81",
        "colab_type": "code",
        "outputId": "fe4e7dac-2247-403e-d077-3a2b5be2543b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "for i in char_dataset.take(14):\n",
        "  print(nums_to_char[i.numpy()])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "z\n",
            "e\n",
            "n\n",
            ":\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbJeIIljM63q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create sequences :) using the batch method\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFvnPr_qNHLL",
        "colab_type": "code",
        "outputId": "d2a688b7-5dee-4fbe-ebbe-ec3eedb98961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(repr(\"\".join(nums_to_char[seq.numpy()])))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPnTO4KsNa4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's split between the input and the predicted text \n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1] # all except the last one\n",
        "  target_text = chunk[1:] # all except the first one\n",
        "  return input_text, target_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QVD56t0OI0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lSwtZrQOfp9",
        "colab_type": "code",
        "outputId": "aedaa07f-244d-4588-a503-638d538a125e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Print some examples\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input data: \", repr(\"\".join(nums_to_char[input_example.numpy()])))\n",
        "  print(\"Target data: \", repr(\"\".join(nums_to_char[target_example.numpy()])))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G9Rx6nEPPXm",
        "colab_type": "code",
        "outputId": "cb4c7da8-41b9-4711-c2f9-93d45433b7d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "for idx, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "  print(\"Step: {:4d}\".format(idx))\n",
        "  print(\" input: {} ({:s})\".format(input_idx, repr(nums_to_char[input_idx])))\n",
        "  print(\" expected output: {} ({:s})\".format(target_idx, repr(nums_to_char[target_idx])))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:    0\n",
            " input: 18 ('F')\n",
            " expected output: 47 ('i')\n",
            "Step:    1\n",
            " input: 47 ('i')\n",
            " expected output: 56 ('r')\n",
            "Step:    2\n",
            " input: 56 ('r')\n",
            " expected output: 57 ('s')\n",
            "Step:    3\n",
            " input: 57 ('s')\n",
            " expected output: 58 ('t')\n",
            "Step:    4\n",
            " input: 58 ('t')\n",
            " expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhuNrFGlQS9d",
        "colab_type": "text"
      },
      "source": [
        "# Create training batches\n",
        "\n",
        "Before building the actual model, let's shuffle the data and pack it into batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktPDWmgfQb_b",
        "colab_type": "code",
        "outputId": "feb7e259-43c9-4e57-f9f1-23eaf5b52717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Batch sizevocab_size\n",
        "BATCH_SIZE = 64 \n",
        "\n",
        "'''\n",
        "Buffer size to shuffle the dataset \n",
        "TF data work with possibly infinite sequences\n",
        "it doesn't shuffle all the sequence in memory \n",
        "Instead mantains a buffer in which it shuffles elements\n",
        "'''\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7RTAsG0RBFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIM = 256\n",
        "#Number of RNN units \n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPz09z6hROPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential(\n",
        "      [\n",
        "       tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "       tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "       tf.keras.layers.Dense(vocab_size)\n",
        "      ]\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-_tj3GcSKdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size=VOCAB_SIZE,\n",
        "                    embedding_dim=EMBEDDING_DIM,\n",
        "                    rnn_units=rnn_units,\n",
        "                    batch_size=BATCH_SIZE\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msvxzp2yoo2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2f6122aa-4ebb-4038-87c8-7c04233ab609"
      },
      "source": [
        "# Let's check the shape of the output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(\"(batch_size, seq_length, vocab_size)\")\n",
        "  print(example_batch_predictions.shape)\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(batch_size, seq_length, vocab_size)\n",
            "(64, 100, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvR77varpQSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "85ea68c2-151b-4e00-8f0a-e2f265247511"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8_nd7b5pwwl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e876aeb9-a8f1-46bb-b116-5d6dceab8cac"
      },
      "source": [
        "# Try out the frist example on the batch \n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "print(sampled_indices.shape)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4obgkn6qdw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "24efb769-885c-4af6-8c3f-1ad84bad63a2"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([48, 44,  6, 37, 39, 12, 26, 64, 12, 20, 61, 54, 36, 61, 23, 10, 57,\n",
              "       26, 12, 63, 10, 59, 49, 45, 23, 20, 43,  6, 21, 31, 49, 30, 16,  4,\n",
              "       25, 56, 23, 20,  9, 46, 18, 28, 22, 51, 22, 23,  4, 26,  0, 50, 28,\n",
              "       63, 58,  0, 27, 57, 34, 46, 52, 40, 40, 29, 26, 43, 64, 47, 39, 23,\n",
              "        1,  4, 24, 37, 29, 33, 57,  7,  6, 50, 16, 33, 20,  7, 24,  3, 44,\n",
              "       18,  9, 20, 29, 22, 56,  7, 40, 37,  4, 10, 11, 15, 33,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUi3b3KUqgf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "7c6b29ce-3e37-4a1b-c23c-6fd93cd294f2"
      },
      "source": [
        "# Decode the text predicted by the untrained model\n",
        "print(\"Input: \\n\", repr(\"\".join(nums_to_char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char predictions: \\n\", repr(\"\".join(nums_to_char[sampled_indices])))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"is\\nThe rarest of all women.\\n\\nLEONTES:\\nGo, Cleomenes;\\nYourself, assisted with your honour'd friends,\\n\"\n",
            "\n",
            "Next Char predictions: \n",
            " 'jf,Ya?Nz?HwpXwK:sN?y:ukgKHe,ISkRD&MrKH3hFPJmJK&N\\nlPyt\\nOsVhnbbQNeziaK &LYQUs-,lDUH-L$fF3HQJr-bY&:;CU '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7iqtIygq85W",
        "colab_type": "text"
      },
      "source": [
        "# Train the model.\n",
        "\n",
        "The problem will be treated as a standard classification problem. Given the previous RNN state and the input, predict the class of next character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxh4JwemrPp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO6is_phrduX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "58d03219-fc31-47c1-f52b-0bd828dc6890"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \"#(batch_size, seq_length, vocab_size)\")\n",
        "print(\"Scalar loss       \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65) #(batch_size, seq_length, vocab_size)\n",
            "Scalar loss        4.174521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76undDjjr4mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure the  model \n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN3-7EddsHNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure checkpoints to ensure those are saved during training \n",
        "CHECKPOINT_DIR = \"./training_checkpoints\"\n",
        "\n",
        "# Name of checkpoint files\n",
        "checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"ckpt_shake_{epoch}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2bwjKItsjFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwB58d9ls028",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Amount of epochs to train for\n",
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUi6Kzu3s5UU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "0f6de4fa-068a-4e95-e0be-ba5f1649438c"
      },
      "source": [
        "# Training step\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 26s 150ms/step - loss: 2.6559\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 24s 140ms/step - loss: 1.9508\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 24s 140ms/step - loss: 1.6842\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 24s 141ms/step - loss: 1.5394\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 24s 141ms/step - loss: 1.4538\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 24s 141ms/step - loss: 1.3932\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 24s 141ms/step - loss: 1.3483\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 24s 141ms/step - loss: 1.3098\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 24s 141ms/step - loss: 1.2753\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 24s 141ms/step - loss: 1.2420\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuDr9IJmtJg0",
        "colab_type": "text"
      },
      "source": [
        "# Generate Text \n",
        "\n",
        "Restore the last checkpoint stored.\n",
        "\n",
        "We will keep the prediction step simple, and will use a batch size of 1 \n",
        "\n",
        "As the RNN state is pased from timestep to timestep, the model only accepts a fixed batch size once is built \n",
        "\n",
        "So we need to rebuild and restore the weights from the checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJLyFPZcu68q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "540ceaf1-8341-4328-87a2-fced46ab415c"
      },
      "source": [
        "tf.train.latest_checkpoint(CHECKPOINT_DIR) # Find the name of the latest saved checkpoint"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_shake_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9XH5v6jvMbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rebuild the model by loading the weights from latest checkpoint\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
        "\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwF-0fwYw2Zq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "c97742f9-3c13-43de-ac34-a2536abe6d99"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YaHyyKkw7_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step ( generating text from the learned model)\n",
        "\n",
        "  # Number of chars to generate \n",
        "  num_to_generate = 1000\n",
        "\n",
        "  # Convert start_string to numbers ( vectorizing step )\n",
        "  input_eval = [chars_to_nums[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store the results\n",
        "  text_generated = [] \n",
        "\n",
        "  # Low temperature results in more predictable text \n",
        "  # Higher temperature results in more surprising text \n",
        "  # Experiment to find the best setting \n",
        "  temperature = 1.0 \n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_to_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # Use a categorical distribution to preduct the word returned by the model \n",
        "    predictions = predictions / temperature \n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # Pass the predicted word as the next input to the model along with the previous \n",
        "    # hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(nums_to_char[predicted_id])\n",
        "  return(start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rphU-FY1yvsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "outputId": "f8952e79-3359-42c8-a80a-60f837d7c578"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: I say, eat, and God Sir Margaret\n",
            "Phall be in our aptemplay'd my brother ill?\n",
            "\n",
            "MARIANA:\n",
            "Well strong as it.\n",
            "\n",
            "KING HENRY VI:\n",
            "Haw not, my gate! 'pay you?\n",
            "\n",
            "MortAR:\n",
            "I know his cousin Ence, your queen, do not\n",
            "Until well assust my butcher blior.\n",
            "I bide my blood is love from Rome,\n",
            "O'ercount we will not be so above to Claudio,\n",
            "And bade my father day love our subjects?\n",
            "\n",
            "WISTY:\n",
            "Yes, I pray, and I, with a man do ait\n",
            "But one had I repore myself to have a pite\n",
            "Is my acquit her to be much,\n",
            "If you were in consent too? dwells, the letters ffort in your died.\n",
            "\n",
            "NASTINCE:\n",
            "His new-made brief, how you must so: But soul,\n",
            "You shoul dineath from the worst cunnot do;\n",
            "For I am too good.\n",
            "\n",
            "CALIBLIHA:\n",
            "No, no; but I cut from maids?\n",
            "\n",
            "ANTONIO:\n",
            "God welcome, thy troth, I tall your grace and leave unto a balt.\n",
            "\n",
            "GHOMSO, hold of that hath fonethy thing.\n",
            "\n",
            "LADY CAPULET:\n",
            "That is my wounded, sir, you know.\n",
            "O, force actionanted myself.\n",
            "\n",
            "CAMILLO:\n",
            "Should thy fault so brief, one foul sorrow\n",
            "In the discords of my business welcome hi\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}